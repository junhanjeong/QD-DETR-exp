import streamlit as st
import json
import pandas as pd
import numpy as np
from googletrans import Translator
import re
from datetime import datetime
import time
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import sys
import os

# Add the parent directory to Python path to import from standalone_eval
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from standalone_eval.utils import compute_average_precision_detection, compute_temporal_iou_batch_cross, compute_temporal_iou_batch_paired

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="QD-DETR Model Comparison Viewer", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-container {
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin-bottom: 1rem;
        background-color: #fafafa;
    }
    .comparison-header {
        background: linear-gradient(90deg, #ff6b6b 0%, #feca57 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 1rem;
    }
    .model1-better {
        background-color: #e8f5e8;
        border-left: 4px solid #4caf50;
        padding: 1rem;
        border-radius: 5px;
        margin-bottom: 1rem;
    }
    .model2-better {
        background-color: #ffebee;
        border-left: 4px solid #f44336;
        padding: 1rem;
        border-radius: 5px;
        margin-bottom: 1rem;
    }
    .query-container {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #2196f3;
        margin-bottom: 1rem;
    }
    .prediction-container {
        background-color: #fff3e0;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #ff9800;
        margin-bottom: 1rem;
    }
    .ground-truth-container {
        background-color: #e8f5e8;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #4caf50;
        margin-bottom: 1rem;
    }
    .performance-diff-high {
        background-color: #c8e6c9;
        border-left: 4px solid #4caf50;
    }
    .performance-diff-medium {
        background-color: #fff9c4;
        border-left: 4px solid #ff9800;
    }
    .performance-diff-low {
        background-color: #ffcdd2;
        border-left: 4px solid #f44336;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_jsonl_data(file_path):
    """JSONL íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line.strip()))
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
    return data

def calculate_iou(pred_window, gt_window):
    """ë‘ ìœˆë„ìš° ê°„ì˜ IoUë¥¼ ê³„ì‚° (eval.pyì™€ ë™ì¼í•œ ë°©ì‹)"""
    pred_start, pred_end = pred_window[0], pred_window[1]
    gt_start, gt_end = gt_window[0], gt_window[1]
    
    # compute_temporal_iou_batch_pairedì™€ ë™ì¼í•œ ë¡œì§
    intersection = max(0, min(pred_end, gt_end) - max(pred_start, gt_start))
    union = max(pred_end, gt_end) - min(pred_start, gt_start)
    
    if union == 0:
        return 0.0
    
    return intersection / union

def calculate_recall_at_1(pred_windows, gt_windows, iou_threshold=0.5):
    """Recall@1 ê³„ì‚° (eval.pyì™€ ë™ì¼í•œ ë°©ì‹)"""
    if not gt_windows or not pred_windows:
        return 0.0
    
    # ìƒìœ„ 1ê°œ ì˜ˆì¸¡ ìœˆë„ìš°ë§Œ ì„ íƒ
    top_pred = pred_windows[0][:2]  # [start, end]ë§Œ ì‚¬ìš©
    
    # GT ìœˆë„ìš°ë“¤ê³¼ IoU ê³„ì‚°í•˜ì—¬ ìµœëŒ€ê°’ êµ¬í•˜ê¸° (eval.pyì˜ compute_mr_r1ê³¼ ë™ì¼)
    pred_array = np.array([top_pred])
    gt_array = np.array(gt_windows)
    ious = compute_temporal_iou_batch_cross(pred_array, gt_array)[0]
    max_iou = np.max(ious)
    
    return 1.0 if max_iou >= iou_threshold else 0.0

def calculate_ap_using_official_method(pred_windows, gt_windows, iou_thresholds=None):
    """eval.pyì˜ compute_average_precision_detectionì„ ì‚¬ìš©í•œ AP ê³„ì‚°"""
    if iou_thresholds is None:
        iou_thresholds = [0.5, 0.75]
    
    if not gt_windows or not pred_windows:
        return {f'AP@{th}': 0.0 for th in iou_thresholds}
    
    # ë°ì´í„°ë¥¼ eval.py formatìœ¼ë¡œ ë³€í™˜
    ground_truth = []
    for i, gt_window in enumerate(gt_windows):
        ground_truth.append({
            'video-id': 'dummy_id',
            't-start': gt_window[0],
            't-end': gt_window[1]
        })
    
    prediction = []
    for pred_window in pred_windows:
        prediction.append({
            'video-id': 'dummy_id',
            't-start': pred_window[0],
            't-end': pred_window[1],
            'score': pred_window[2] if len(pred_window) > 2 else 1.0
        })
    
    ap_results = {}
    
    # ê° IoU thresholdì— ëŒ€í•´ AP ê³„ì‚°
    for iou_threshold in iou_thresholds:
        ap_scores = compute_average_precision_detection(
            ground_truth, 
            prediction, 
            tiou_thresholds=np.array([iou_threshold])
        )
        ap_results[f'AP@{iou_threshold}'] = ap_scores[0]
    
    return ap_results

def calculate_ap(pred_windows, gt_windows, iou_thresholds=None):
    """eval.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ AP ê³„ì‚°"""
    if iou_thresholds is None:
        iou_thresholds = [0.5, 0.75]
    
    # ê¸°ë³¸ AP@0.5, AP@0.75 ê³„ì‚°
    ap_results = calculate_ap_using_official_method(pred_windows, gt_windows, iou_thresholds)
    
    # AP@Avg ê³„ì‚° (0.5ë¶€í„° 0.95ê¹Œì§€ 10ê°œ thresholdì˜ í‰ê· )
    if iou_thresholds == [0.5, 0.75]:  # ê¸°ë³¸ í˜¸ì¶œì¸ ê²½ìš°ì—ë§Œ AP@Avg ê³„ì‚°
        avg_thresholds = np.linspace(0.5, 0.95, 10)
        ap_avg_results = calculate_ap_using_official_method(pred_windows, gt_windows, avg_thresholds)
        avg_ap_values = [ap_avg_results[f'AP@{th}'] for th in avg_thresholds]
        ap_results['AP@Avg'] = np.mean(avg_ap_values)
    
    return ap_results
    
    return ap_results

@st.cache_data
def process_predictions(pred_data, gt_data):
    """ì˜ˆì¸¡ ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ ë§¤ì¹­í•˜ê³  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ìºì‹œë¨)"""
    results = []
    
    # GT ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    gt_dict = {item['qid']: item for item in gt_data}
    
    for pred_item in pred_data:
        qid = pred_item['qid']
        
        if qid not in gt_dict:
            continue
        
        gt_item = gt_dict[qid]
        
        # ì˜ˆì¸¡ ìœˆë„ìš° íŒŒì‹±
        pred_windows = pred_item.get('pred_relevant_windows', [])
        
        # ì‹¤ì œ ìœˆë„ìš° íŒŒì‹±
        gt_windows = gt_item.get('relevant_windows', [])
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        recall_1_05 = calculate_recall_at_1(pred_windows, gt_windows, iou_threshold=0.5)
        recall_1_07 = calculate_recall_at_1(pred_windows, gt_windows, iou_threshold=0.7)
        ap_scores = calculate_ap(pred_windows, gt_windows, iou_thresholds=[0.5, 0.75])
        
        result = {
            'qid': qid,
            'query': pred_item.get('query', ''),
            'vid': pred_item.get('vid', ''),
            'duration': gt_item.get('duration', 0),
            'pred_windows': pred_windows,
            'gt_windows': gt_windows,
            'recall_1_05': recall_1_05,
            'recall_1_07': recall_1_07,
            'ap_05': ap_scores['AP@0.5'],
            'ap_075': ap_scores['AP@0.75'],
            'ap_avg': ap_scores['AP@Avg'],
            'pred_saliency_scores': pred_item.get('pred_saliency_scores', [])
        }
        
        results.append(result)
    
    return results

@st.cache_data
def compare_models(results1, results2):
    """ë‘ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë¹„êµ (ìºì‹œë¨)"""
    # ê³µí†µ QID ì°¾ê¸°
    qids1 = {r['qid'] for r in results1}
    qids2 = {r['qid'] for r in results2}
    common_qids = qids1.intersection(qids2)
    
    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    results1_dict = {r['qid']: r for r in results1}
    results2_dict = {r['qid']: r for r in results2}
    
    comparisons = []
    
    for qid in common_qids:
        r1 = results1_dict[qid]
        r2 = results2_dict[qid]
        
        comparison = {
            'qid': qid,
            'query': r1['query'],
            'vid': r1['vid'],
            'duration': r1['duration'],
            'gt_windows': r1['gt_windows'],
            'model1_ap_avg': r1['ap_avg'],
            'model2_ap_avg': r2['ap_avg'],
            'ap_diff': r1['ap_avg'] - r2['ap_avg'],  # Model1 - Model2
            'model1_pred_windows': r1['pred_windows'],
            'model2_pred_windows': r2['pred_windows'],
            'model1_saliency': r1['pred_saliency_scores'],
            'model2_saliency': r2['pred_saliency_scores'],
            'model1_recall_1_05': r1['recall_1_05'],
            'model2_recall_1_05': r2['recall_1_05'],
            'model1_recall_1_07': r1['recall_1_07'],
            'model2_recall_1_07': r2['recall_1_07'],
            'model1_ap_05': r1['ap_05'],
            'model2_ap_05': r2['ap_05'],
            'model1_ap_075': r1['ap_075'],
            'model2_ap_075': r2['ap_075'],
        }
        
        comparisons.append(comparison)
    
    return comparisons

def get_youtube_embed_url(youtube_id, start_time=None):
    """YouTube ì„ë² ë“œ URLì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if youtube_id:
        base_url = f"https://www.youtube.com/embed/{youtube_id}"
        if start_time:
            base_url += f"?start={int(start_time)}&autoplay=1"
        return base_url
    return None

def seconds_to_mmss(seconds):
    """ì´ˆë¥¼ mm:ss í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    minutes = int(seconds // 60)
    seconds = int(seconds % 60)
    return f"{minutes:02d}:{seconds:02d}"

def extract_youtube_id_from_vid(vid):
    """vidì—ì„œ YouTube IDë¥¼ ì¶”ì¶œ"""
    parts = vid.split('_')
    if len(parts) >= 2:
        return parts[0]
    return None

def get_youtube_url(youtube_id):
    """YouTube URL ìƒì„±"""
    if youtube_id:
        return f"https://www.youtube.com/watch?v={youtube_id}"
    return None

def get_video_start_time(vid):
    """vidì—ì„œ ë¹„ë””ì˜¤ ì‹œì‘ ì‹œê°„ì„ ì¶”ì¶œ"""
    parts = vid.split('_')
    if len(parts) >= 2:
        try:
            return float(parts[1])
        except ValueError:
            return 0.0
    return 0.0

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def process_and_compare_models(model1_file, model2_file, gt_file):
    """ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬ì™€ ë¹„êµë¥¼ í•œ ë²ˆì— ìˆ˜í–‰ (ë©”ì¸ ìºì‹œ í•¨ìˆ˜)"""
    # ë°ì´í„° ë¡œë“œ
    model1_data = load_jsonl_data(model1_file)
    model2_data = load_jsonl_data(model2_file)
    gt_data = load_jsonl_data(gt_file)
    
    if not model1_data or not model2_data or not gt_data:
        return None, None, None
    
    # ì„±ëŠ¥ ê³„ì‚°
    results1 = process_predictions(model1_data, gt_data)
    results2 = process_predictions(model2_data, gt_data)
    
    # ëª¨ë¸ ë¹„êµ
    comparisons = compare_models(results1, results2)
    
    # GT ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    gt_data_dict = {item['qid']: item for item in gt_data}
    
    return comparisons, gt_data_dict, len(gt_data)

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def translate_text_cached(text, target_language='ko'):
    """ìºì‹œëœ ë²ˆì—­ í•¨ìˆ˜"""
    try:
        translator = Translator()
        result = translator.translate(text, dest=target_language)
        return result.text
    except Exception as e:
        return f"ë²ˆì—­ ì‹¤íŒ¨: {text}"

@st.cache_data
def plot_comparison_visualization(comparison, gt_data_dict, model1_name, model2_name):
    """ëª¨ë¸ ë¹„êµ ì‹œê°í™” (ìºì‹œë¨)"""
    qid = comparison['qid']
    gt_data_item = gt_data_dict.get(qid, {})
    
    fig = make_subplots(
        rows=6, cols=1,
        subplot_titles=(
            f'{model1_name} Predicted Windows', 
            f'{model2_name} Predicted Windows',
            'Ground Truth Windows', 
            f'{model1_name} Saliency Scores', 
            f'{model2_name} Saliency Scores',
            'Ground Truth Saliency Scores'
        ),
        vertical_spacing=0.08,
        row_heights=[0.15, 0.15, 0.15, 0.18, 0.18, 0.19]
    )
    
    # Model 1 ì˜ˆì¸¡ ìœˆë„ìš° ì‹œê°í™” (ìƒìœ„ 10ê°œ)
    model1_windows = comparison['model1_pred_windows'][:10]
    for i, window in enumerate(model1_windows):
        start, end = window[0], window[1]
        confidence = window[2] if len(window) > 2 else 1.0
        
        fig.add_trace(
            go.Scatter(
                x=[start, end, end, start, start],
                y=[i, i, i+0.8, i+0.8, i],
                fill="toself",
                fillcolor=f"rgba(255, 99, 71, {confidence})",
                line=dict(color="red", width=2),
                name=f"{model1_name} Pred {i+1} ({confidence * 100:.2f})",
                mode="lines",
                showlegend=False
            ),
            row=1, col=1
        )
    
    # Model 2 ì˜ˆì¸¡ ìœˆë„ìš° ì‹œê°í™” (ìƒìœ„ 10ê°œ)
    model2_windows = comparison['model2_pred_windows'][:10]
    for i, window in enumerate(model2_windows):
        start, end = window[0], window[1]
        confidence = window[2] if len(window) > 2 else 1.0
        
        fig.add_trace(
            go.Scatter(
                x=[start, end, end, start, start],
                y=[i, i, i+0.8, i+0.8, i],
                fill="toself",
                fillcolor=f"rgba(54, 162, 235, {confidence})",
                line=dict(color="blue", width=2),
                name=f"{model2_name} Pred {i+1} ({confidence * 100:.2f})",
                mode="lines",
                showlegend=False
            ),
            row=2, col=1
        )
    
    # ì‹¤ì œ ìœˆë„ìš° ì‹œê°í™”  
    gt_windows = comparison['gt_windows']
    for i, window in enumerate(gt_windows):
        start, end = window[0], window[1]
        
        fig.add_trace(
            go.Scatter(
                x=[start, end, end, start, start],
                y=[i, i, i+0.8, i+0.8, i],
                fill="toself",
                fillcolor="rgba(50, 205, 50, 0.7)",
                line=dict(color="green", width=2),
                name=f"GT {i+1}",
                mode="lines",
                showlegend=False
            ),
            row=3, col=1
        )
    
    # Model 1 Saliency ì ìˆ˜ ì‹œê°í™”
    model1_saliency = comparison['model1_saliency']
    if model1_saliency:
        time_points = [i * 2 for i in range(len(model1_saliency))]
        fig.add_trace(
            go.Scatter(
                x=time_points,
                y=model1_saliency,
                mode="lines+markers",
                name=f"{model1_name} Saliency",
                line=dict(color="red", width=2),
                marker=dict(size=4),
                showlegend=False
            ),
            row=4, col=1
        )
    
    # Model 2 Saliency ì ìˆ˜ ì‹œê°í™”
    model2_saliency = comparison['model2_saliency']
    if model2_saliency:
        time_points = [i * 2 for i in range(len(model2_saliency))]
        fig.add_trace(
            go.Scatter(
                x=time_points,
                y=model2_saliency,
                mode="lines+markers",
                name=f"{model2_name} Saliency",
                line=dict(color="blue", width=2),
                marker=dict(size=4),
                showlegend=False
            ),
            row=5, col=1
        )
    
    # GT Saliency ì ìˆ˜ ì‹œê°í™”
    if gt_data_item and 'saliency_scores' in gt_data_item:
        gt_saliency_scores = gt_data_item['saliency_scores']
        if gt_saliency_scores:
            # GT saliency scoresë¥¼ í‰ê· ê°’ìœ¼ë¡œ ê³„ì‚°
            avg_gt_saliency = [np.mean(scores) for scores in gt_saliency_scores]
            
            # relevant_clip_idsê°€ ìˆìœ¼ë©´ í•´ë‹¹ time points ì‚¬ìš©, ì—†ìœ¼ë©´ ì—°ì†ì ìœ¼ë¡œ ìƒì„±
            if 'relevant_clip_ids' in gt_data_item and gt_data_item['relevant_clip_ids']:
                time_points_gt = [clip_id * 2 for clip_id in gt_data_item['relevant_clip_ids']]
            else:
                time_points_gt = [i * 2 for i in range(len(avg_gt_saliency))]
            
            fig.add_trace(
                go.Scatter(
                    x=time_points_gt,
                    y=avg_gt_saliency,
                    mode="lines+markers",
                    name="GT Saliency (avg)",
                    line=dict(color="green", width=2),
                    marker=dict(size=4),
                    showlegend=False
                ),
                row=6, col=1
            )
    
    fig.update_layout(
        height=1200,
        title_text=f"Model Comparison: {model1_name} vs {model2_name}",
        showlegend=False,
        margin=dict(l=50, r=50, t=100, b=50)
    )
    
    # xì¶• ë²”ìœ„ ì„¤ì • (ìœˆë„ìš°ëŠ” 0~150, saliencyëŠ” 0~75)
    for row in [1, 2, 3]:
        fig.update_xaxes(range=[0, 150], title_text="Time (seconds)", row=row, col=1)
    for row in [4, 5, 6]:
        fig.update_xaxes(range=[0, 75], title_text="Time (seconds)", row=row, col=1)
    
    # yì¶• ì œëª© ì„¤ì •
    for row in [1, 2, 3]:
        fig.update_yaxes(title_text="Windows", row=row, col=1)
    for row in [4, 5, 6]:
        fig.update_yaxes(title_text="Saliency", row=row, col=1)
    
    return fig

def display_comparison_item(comparison, idx, model1_name, model2_name, gt_data_dict):
    """ë¹„êµ ê²°ê³¼ í•­ëª©ì„ í‘œì‹œ"""
    qid = comparison['qid']
    query = comparison['query']
    vid = comparison['vid']
    duration = comparison['duration']
    gt_windows = comparison['gt_windows']
    
    model1_ap_avg = comparison['model1_ap_avg']
    model2_ap_avg = comparison['model2_ap_avg']
    ap_diff = comparison['ap_diff']
    
    # YouTube ì •ë³´
    youtube_id = extract_youtube_id_from_vid(vid)
    youtube_url = get_youtube_url(youtube_id)
    video_start_time = get_video_start_time(vid)
    
    # ì„±ëŠ¥ ì°¨ì´ ë¶„ë¥˜
    if ap_diff >= 0.3:
        perf_class = "performance-diff-high"
        perf_emoji = "ğŸŸ¢"
        diff_desc = f"{model1_name} ìš°ìœ„"
    elif ap_diff >= 0.1:
        perf_class = "performance-diff-medium"
        perf_emoji = "ğŸŸ¡"
        diff_desc = f"{model1_name} ì•½ê°„ ìš°ìœ„"
    elif ap_diff <= -0.3:
        perf_class = "performance-diff-low"
        perf_emoji = "ğŸ”´"
        diff_desc = f"{model2_name} ìš°ìœ„"
    elif ap_diff <= -0.1:
        perf_class = "performance-diff-medium"
        perf_emoji = "ğŸŸ¡"
        diff_desc = f"{model2_name} ì•½ê°„ ìš°ìœ„"
    else:
        perf_class = "performance-diff-medium"
        perf_emoji = "âšª"
        diff_desc = "ë¹„ìŠ·í•œ ì„±ëŠ¥"
    
    # ë©”ì¸ í—¤ë”
    st.markdown(f"""
    <div class="comparison-header">
        <h3>{perf_emoji} ì¿¼ë¦¬ #{idx} (QID: {qid}) - AP@Avg ì°¨ì´: {ap_diff * 100:.2f} ({diff_desc})</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # ì»¬ëŸ¼ ë¶„í• 
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # ì›ë³¸ ì¿¼ë¦¬
        st.markdown(f"""
        <div class="query-container">
            <h4>ğŸ” ì›ë³¸ ì¿¼ë¦¬</h4>
            <p>{query}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # ë²ˆì—­ëœ ì¿¼ë¦¬
        with st.spinner("ë²ˆì—­ ì¤‘..."):
            translated_query = translate_text_cached(query, 'ko')
        
        st.markdown(f"""
        <div class="query-container">
            <h4>ğŸŒ í•œêµ­ì–´ ë²ˆì—­</h4>
            <p>{translated_query}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # ëª¨ë¸ ë¹„êµ ì„±ëŠ¥ ì§€í‘œ
        st.markdown(f"#### ğŸ“Š ì„±ëŠ¥ ë¹„êµ: {model1_name} vs {model2_name}")
        
        # AP@Avg ë¹„êµ
        col_ap1, col_ap2, col_diff = st.columns(3)
        
        with col_ap1:
            st.metric(
                f"{model1_name} AP@Avg", 
                f"{model1_ap_avg * 100:.2f}",
                help=f"{model1_name}ì˜ Average Precision"
            )
        
        with col_ap2:
            st.metric(
                f"{model2_name} AP@Avg", 
                f"{model2_ap_avg * 100:.2f}",
                help=f"{model2_name}ì˜ Average Precision"
            )
        
        with col_diff:
            delta_color = "normal" if abs(ap_diff) < 0.1 else ("inverse" if ap_diff < 0 else "normal")
            st.metric(
                "ì°¨ì´ (1-2)", 
                f"{ap_diff * 100:.2f}",
                delta=f"{ap_diff * 100:.2f}",
                help="Model1 - Model2ì˜ ì„±ëŠ¥ ì°¨ì´"
            )
        
        # ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œë“¤
        st.markdown("##### ì„¸ë¶€ ì„±ëŠ¥ ì§€í‘œ")
        
        metrics_data = {
            "ì§€í‘œ": ["Recall@1 (0.5)", "Recall@1 (0.7)", "AP@0.5", "AP@0.75"],
            f"{model1_name}": [
                f"{comparison['model1_recall_1_05'] * 100:.2f}",
                f"{comparison['model1_recall_1_07'] * 100:.2f}",
                f"{comparison['model1_ap_05'] * 100:.2f}",
                f"{comparison['model1_ap_075'] * 100:.2f}"
            ],
            f"{model2_name}": [
                f"{comparison['model2_recall_1_05'] * 100:.2f}",
                f"{comparison['model2_recall_1_07'] * 100:.2f}",
                f"{comparison['model2_ap_05'] * 100:.2f}",
                f"{comparison['model2_ap_075'] * 100:.2f}"
            ]
        }
        
        metrics_df = pd.DataFrame(metrics_data)
        st.dataframe(metrics_df, use_container_width=True)
        
        # ë¹„ë””ì˜¤ ì •ë³´
        st.markdown(f"""
        <div class="video-info">
            <h4>ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´</h4>
            <p><strong>VID:</strong> <code>{vid}</code></p>
            <p><strong>YouTube ID:</strong> <code>{youtube_id}</code></p>
            <p><strong>ë¹„ë””ì˜¤ ê¸¸ì´:</strong> {duration}ì´ˆ</p>
            <p><strong>YouTube ë§í¬:</strong> <a href="{youtube_url}" target="_blank">ì—¬ê¸°ì„œ ë³´ê¸°</a></p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        # YouTube ë¹„ë””ì˜¤ ì„ë² ë“œ
        if youtube_id:
            st.markdown("#### ğŸ¥ YouTube ë¹„ë””ì˜¤")
            
            # VIDì˜ ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ë¹„ë””ì˜¤ ì‹œì‘
            embed_start_time = video_start_time
            
            embed_url = get_youtube_embed_url(youtube_id, embed_start_time)
            
            # iframeì„ ì‚¬ìš©í•˜ì—¬ YouTube ë¹„ë””ì˜¤ ì„ë² ë“œ
            st.markdown(f"""
            <iframe width="100%" height="315" 
                    src="{embed_url}" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
            </iframe>
            """, unsafe_allow_html=True)
            
            # ì‹œê°„ ì í”„ ë²„íŠ¼ë“¤
            if gt_windows:
                st.markdown("**â¯ï¸ êµ¬ê°„ ë°”ë¡œê°€ê¸°:**")
                for j, window in enumerate(gt_windows):
                    if len(window) >= 2:
                        jump_time = video_start_time + window[0]
                        jump_url = get_youtube_url(youtube_id) + f"&t={int(jump_time)}s"
                        st.markdown(f"ğŸ”— [êµ¬ê°„ {j+1} ë°”ë¡œê°€ê¸°]({jump_url})")
        else:
            st.warning("âš ï¸ YouTube ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì •ë‹µ ëª¨ë©˜íŠ¸ êµ¬ê°„
        st.markdown("**â° ì •ë‹µ ëª¨ë©˜íŠ¸ êµ¬ê°„:**")
        
        if gt_windows:
            for j, window in enumerate(gt_windows):
                if len(window) >= 2:
                    start_in_clip = window[0]
                    end_in_clip = window[1]
                    
                    # ì „ì²´ ë¹„ë””ì˜¤ì—ì„œì˜ ì‹¤ì œ ì‹œê°„ ê³„ì‚°
                    actual_start = video_start_time + start_in_clip
                    actual_end = video_start_time + end_in_clip
                    
                    st.markdown(f"""
                    <div class="ground-truth-container">
                        <strong>êµ¬ê°„ {j+1}:</strong><br>
                        &nbsp;&nbsp;â€¢ í´ë¦½ ë‚´: {seconds_to_mmss(start_in_clip)} ~ {seconds_to_mmss(end_in_clip)}<br>
                        &nbsp;&nbsp;â€¢ ì „ì²´ ì˜ìƒ: {seconds_to_mmss(actual_start)} ~ {seconds_to_mmss(actual_end)}
                    </div>
                    """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="ground-truth-container">
                ì •ë‹µ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.
            </div>
            """, unsafe_allow_html=True)
    
    # ì‹œê°„ì¶• ì‹œê°í™”
    fig = plot_comparison_visualization(comparison, gt_data_dict, model1_name, model2_name)
    st.plotly_chart(fig, use_container_width=True)
    
    # ìƒì„¸ ì˜ˆì¸¡ ì •ë³´ ë¹„êµ
    st.markdown(f"#### ğŸ“‹ ëª¨ë¸ ì˜ˆì¸¡ ë¹„êµ (ìƒìœ„ 5ê°œì”©)")
    
    col_m1, col_m2 = st.columns(2)
    
    with col_m1:
        st.markdown(f"**{model1_name} ì˜ˆì¸¡**")
        for i, window in enumerate(comparison['model1_pred_windows'][:5]):
            start, end = window[0], window[1]
            confidence = window[2] if len(window) > 2 else 1.0
            
            # ì „ì²´ ë¹„ë””ì˜¤ì—ì„œì˜ ì‹¤ì œ ì‹œê°„
            actual_start = video_start_time + start
            actual_end = video_start_time + end
            
            st.markdown(f"""
            <div class="model1-better">
                <strong>ì˜ˆì¸¡ {i+1}:</strong> {seconds_to_mmss(start)} ~ {seconds_to_mmss(end)} 
                (ì‹ ë¢°ë„: {confidence * 100:.2f})<br>
                <em>ì „ì²´ ì˜ìƒ: {seconds_to_mmss(actual_start)} ~ {seconds_to_mmss(actual_end)}</em>
            </div>
            """, unsafe_allow_html=True)
    
    with col_m2:
        st.markdown(f"**{model2_name} ì˜ˆì¸¡**")
        for i, window in enumerate(comparison['model2_pred_windows'][:5]):
            start, end = window[0], window[1]
            confidence = window[2] if len(window) > 2 else 1.0
            
            # ì „ì²´ ë¹„ë””ì˜¤ì—ì„œì˜ ì‹¤ì œ ì‹œê°„
            actual_start = video_start_time + start
            actual_end = video_start_time + end
            
            st.markdown(f"""
            <div class="model2-better">
                <strong>ì˜ˆì¸¡ {i+1}:</strong> {seconds_to_mmss(start)} ~ {seconds_to_mmss(end)} 
                (ì‹ ë¢°ë„: {confidence * 100:.2f})<br>
                <em>ì „ì²´ ì˜ìƒ: {seconds_to_mmss(actual_start)} ~ {seconds_to_mmss(actual_end)}</em>
            </div>
            """, unsafe_allow_html=True)

def main():
    # ë©”ì¸ í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ”„ QD-DETR Model Comparison Viewer</h1>
        <p>ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ì°¨ì´ë¥¼ ë¶„ì„í•˜ì„¸ìš”</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("âš™ï¸ ì„¤ì •")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        model1_file = st.text_input(
            "ğŸ¤– ëª¨ë¸ 1 ì˜ˆì¸¡ íŒŒì¼:",
            value="../../results/hl-video_tef-audio_experiment-2025_07_04_12_02_18/best_hl_val_preds.jsonl",
            help="ëª¨ë¸ 1ì˜ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ"
        )
        
        model1_name = st.text_input(
            "ğŸ“ ëª¨ë¸ 1 ì´ë¦„:",
            value="Model 1",
            help="ëª¨ë¸ 1ì˜ í‘œì‹œ ì´ë¦„"
        )
        
        model2_file = st.text_input(
            "ğŸ¤– ëª¨ë¸ 2 ì˜ˆì¸¡ íŒŒì¼:",
            value="../../results/hl-video_tef-audio_only-2025_07_05_12_22_33/best_hl_val_preds.jsonl",
            help="ëª¨ë¸ 2ì˜ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ"
        )
        
        model2_name = st.text_input(
            "ğŸ“ ëª¨ë¸ 2 ì´ë¦„:",
            value="Modlel 2",
            help="ëª¨ë¸ 2ì˜ í‘œì‹œ ì´ë¦„"
        )
        
        gt_file = st.text_input(
            "âœ… ì‹¤ì œ ì •ë‹µ íŒŒì¼:",
            value="../../data/highlight_val_release.jsonl",
            help="Validation ì •ë‹µ ë°ì´í„° íŒŒì¼ ê²½ë¡œ"
        )
        
        # í•„í„° ì˜µì…˜
        comparison_type = st.selectbox(
            "ğŸ¯ ë¹„êµ íƒ€ì…:",
            ["Model1ì´ ë” ì¢‹ì€ ê²½ìš°", "Model2ê°€ ë” ì¢‹ì€ ê²½ìš°", "ëª¨ë“  ë¹„êµ"],
            help="ì–´ë–¤ ë¹„êµ ê²°ê³¼ë¥¼ ë³´ê³  ì‹¶ì€ì§€ ì„ íƒí•˜ì„¸ìš”"
        )
        
        min_diff = st.slider(
            "ğŸ“Š ìµœì†Œ ì„±ëŠ¥ ì°¨ì´ (AP@Avg):",
            min_value=0.0,
            max_value=100.0,
            value=10.0,
            step=1.0,
            help="ì´ ê°’ ì´ìƒì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ê°€ì§„ ì¿¼ë¦¬ë§Œ í‘œì‹œ (ë°±ë¶„ìœ¨)"
        )
        
        # í˜ì´ì§€ë‹¹ ì•„ì´í…œ ìˆ˜
        items_per_page = st.slider(
            "ğŸ“„ í˜ì´ì§€ë‹¹ ì•„ì´í…œ ìˆ˜:",
            min_value=1,
            max_value=10,
            value=1
        )
        
        # ê²€ìƒ‰
        search_query = st.text_input(
            "ğŸ” ì¿¼ë¦¬ ê²€ìƒ‰:",
            placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
            help="ì¿¼ë¦¬ ë‚´ìš©ìœ¼ë¡œ ê²€ìƒ‰"
        )
    
    # ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ (ìºì‹œë¨)
    with st.spinner("ğŸ“Š ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì„±ëŠ¥ì„ ê³„ì‚°í•˜ëŠ” ì¤‘... (ì²« ë²ˆì§¸ ì‹¤í–‰ ì‹œì—ë§Œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)"):
        result = process_and_compare_models(model1_file, model2_file, gt_file)
        
        if result[0] is None:
            st.error("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        
        comparisons, gt_data_dict, total_gt_count = result
    
    if not comparisons:
        st.error("âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì „ì²´ í†µê³„
    model1_avg_ap = np.mean([c['model1_ap_avg'] for c in comparisons])
    model2_avg_ap = np.mean([c['model2_ap_avg'] for c in comparisons])
    avg_diff = np.mean([c['ap_diff'] for c in comparisons])
    
    model1_better_count = sum(1 for c in comparisons if c['ap_diff'] > 0.1)
    model2_better_count = sum(1 for c in comparisons if c['ap_diff'] < -0.1)
    similar_count = sum(1 for c in comparisons if abs(c['ap_diff']) <= 0.1)
    
    st.markdown("#### ğŸ“Š ì „ì²´ ë¹„êµ í†µê³„")
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric(f"{model1_name} í‰ê·  AP@Avg", f"{model1_avg_ap * 100:.2f}")
    with col2:
        st.metric(f"{model2_name} í‰ê·  AP@Avg", f"{model2_avg_ap * 100:.2f}")
    with col3:
        st.metric("í‰ê·  ì°¨ì´", f"{avg_diff * 100:.2f}")
    with col4:
        st.metric(f"{model1_name} ìš°ìœ„", f"{model1_better_count}ê°œ")
    with col5:
        st.metric(f"{model2_name} ìš°ìœ„", f"{model2_better_count}ê°œ")
    
    # ì´ ë¹„êµ ìˆ˜ì™€ ë¹„ìŠ·í•œ ì„±ëŠ¥ ìˆ˜
    col_total, col_similar = st.columns(2)
    with col_total:
        st.metric("ì´ ë¹„êµ ì¿¼ë¦¬ ìˆ˜", len(comparisons))
    with col_similar:
        st.metric("ë¹„ìŠ·í•œ ì„±ëŠ¥", f"{similar_count}ê°œ")
    
    # ì„±ëŠ¥ ì°¨ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (ìºì‹œë¨)
    @st.cache_data
    def create_performance_distribution_plot(comparisons_data):
        fig_dist = go.Figure()
        fig_dist.add_trace(
            go.Histogram(
                x=[c['ap_diff'] * 100 for c in comparisons_data], 
                name="AP@Avg ì°¨ì´ ë¶„í¬", 
                nbinsx=30,
                marker_color='skyblue'
            )
        )
        fig_dist.update_layout(
            title="AP@Avg ì„±ëŠ¥ ì°¨ì´ ë¶„í¬ (Model1 - Model2)",
            xaxis_title="ì„±ëŠ¥ ì°¨ì´ (%)",
            yaxis_title="ì¿¼ë¦¬ ìˆ˜",
            height=400
        )
        return fig_dist
    
    fig_dist = create_performance_distribution_plot(comparisons)
    st.plotly_chart(fig_dist, use_container_width=True)
    
    # ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬
    filtered_comparisons = comparisons
    
    # ë¹„êµ íƒ€ì… í•„í„°
    if comparison_type == "Model1ì´ ë” ì¢‹ì€ ê²½ìš°":
        filtered_comparisons = [c for c in filtered_comparisons if c['ap_diff'] > min_diff / 100]
    elif comparison_type == "Model2ê°€ ë” ì¢‹ì€ ê²½ìš°":
        filtered_comparisons = [c for c in filtered_comparisons if c['ap_diff'] < -min_diff / 100]
    else:  # ëª¨ë“  ë¹„êµ
        filtered_comparisons = [c for c in filtered_comparisons if abs(c['ap_diff']) >= min_diff / 100]
    
    # ê²€ìƒ‰ í•„í„°
    if search_query:
        filtered_comparisons = [
            c for c in filtered_comparisons 
            if search_query.lower() in c['query'].lower()
        ]
    
    # ì •ë ¬ (ì„±ëŠ¥ ì°¨ì´ ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ)
    if comparison_type == "Model1ì´ ë” ì¢‹ì€ ê²½ìš°":
        filtered_comparisons.sort(key=lambda x: x['ap_diff'], reverse=True)
    elif comparison_type == "Model2ê°€ ë” ì¢‹ì€ ê²½ìš°":
        filtered_comparisons.sort(key=lambda x: x['ap_diff'])
    else:
        filtered_comparisons.sort(key=lambda x: abs(x['ap_diff']), reverse=True)
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    total_items = len(filtered_comparisons)
    total_pages = max(1, (total_items - 1) // items_per_page + 1)
    
    if 'page' not in st.session_state:
        st.session_state.page = 1
    
    st.markdown(f"**ğŸ¯ í•„í„°ë§ëœ ê²°ê³¼: {total_items}ê°œ (ì „ì²´ {len(comparisons)}ê°œ ì¤‘)**")
    
    # í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜
    col1, col2, col3, col4, col5 = st.columns([1, 1, 2, 1, 1])
    
    with col1:
        if st.button("â¬…ï¸ ì´ì „", disabled=st.session_state.page <= 1):
            st.session_state.page -= 1
    
    with col2:
        if st.button("â®ï¸ ì²˜ìŒ"):
            st.session_state.page = 1
    
    with col3:
        page_options = list(range(1, total_pages + 1))
        current_page_index = min(st.session_state.page - 1, len(page_options) - 1)
        current_page_index = max(0, current_page_index)
        
        page = st.selectbox(
            "í˜ì´ì§€:",
            page_options,
            index=current_page_index,
            key='page_select'
        )
        if page != st.session_state.page:
            st.session_state.page = page
    
    with col4:
        if st.button("â­ï¸ ë§ˆì§€ë§‰"):
            st.session_state.page = total_pages
    
    with col5:
        if st.button("â¡ï¸ ë‹¤ìŒ", disabled=st.session_state.page >= total_pages):
            st.session_state.page += 1
    
    # í˜„ì¬ í˜ì´ì§€ ë°ì´í„°
    start_idx = (st.session_state.page - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, total_items)
    current_comparisons = filtered_comparisons[start_idx:end_idx]
    
    st.markdown(f"**ğŸ“Š í˜ì´ì§€ {st.session_state.page} / {total_pages}** (ì•„ì´í…œ {start_idx + 1}-{end_idx} / {total_items})")
    st.markdown("---")
    
    # ê²°ê³¼ í‘œì‹œ
    for i, comparison in enumerate(current_comparisons):
        actual_idx = start_idx + i + 1
        display_comparison_item(comparison, actual_idx, model1_name, model2_name, gt_data_dict)
        
        if i < len(current_comparisons) - 1:
            st.markdown("---")

if __name__ == "__main__":
    main()
